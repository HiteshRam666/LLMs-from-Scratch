{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgNOpIz1aoHFH+mV3dVKZZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"0XmWETiYgzw0","executionInfo":{"status":"ok","timestamp":1738294172803,"user_tz":-330,"elapsed":469,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"outputs":[],"source":["with open(\"the-verdict.txt\", \"r\", encoding = 'utf-8') as f:\n","  raw_text = f.read()"]},{"cell_type":"code","source":["print(f\"Length of Raw Text: {len(raw_text)}\")\n","print(raw_text[:99])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bAoVAtYIg_rX","executionInfo":{"status":"ok","timestamp":1738294175988,"user_tz":-330,"elapsed":1697,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"17649edb-93e6-4d8d-aebb-43e996101ba4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of Raw Text: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}]},{"cell_type":"code","source":["import re\n","text = \"Hello world, This is test word.\"\n","# \\s is a pattern for whitespace\n","result = re.split(r'(\\s)', text)\n","\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gt7tZ0BGhFBJ","executionInfo":{"status":"ok","timestamp":1738294175988,"user_tz":-330,"elapsed":35,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"5522f195-1ed6-4024-b055-e10464919b6c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ' ', 'world,', ' ', 'This', ' ', 'is', ' ', 'test', ' ', 'word.']\n"]}]},{"cell_type":"code","source":["# Splitting punctuations too from the word\n","\n","result = re.split(r'([,.]|\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aQHKOttiYhY","executionInfo":{"status":"ok","timestamp":1738294175988,"user_tz":-330,"elapsed":32,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"3c73caea-f932-4832-9bb8-e42f2626d8c7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ' ', 'world', ',', '', ' ', 'This', ' ', 'is', ' ', 'test', ' ', 'word', '.', '']\n"]}]},{"cell_type":"code","source":["# Removing whitespaces which are still present in the result\n","result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_1TkUu5jAb3","executionInfo":{"status":"ok","timestamp":1738294175989,"user_tz":-330,"elapsed":30,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"92d83fdd-a123-4899-e5ed-7ee4379c1d27"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'world', ',', 'This', 'is', 'test', 'word', '.']\n"]}]},{"cell_type":"markdown","source":["The tokenization scheme we devised above works well on the simple sample text. Let's\n","modify it a bit further so that it can also handle other types of punctuation, such as\n","question marks, quotation marks, and the double-dashes"],"metadata":{"id":"fPVMNRtXkiLe"}},{"cell_type":"code","source":["text = \"Hello world--, Welcome. to! Python? programming'\"\n","\n","result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","result = [item.strip() for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzvqSQMYjRfU","executionInfo":{"status":"ok","timestamp":1738294175989,"user_tz":-330,"elapsed":28,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"5950bd26-6abc-4d20-f13b-a476d8e9c381"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'world', '--', ',', 'Welcome', '.', 'to', '!', 'Python', '?', 'programming', \"'\"]\n"]}]},{"cell_type":"markdown","source":["###Applying on our loaded raw text"],"metadata":{"id":"tXuspBnsoN8A"}},{"cell_type":"code","source":["processed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","processed = [item.strip() for item in processed if item.strip()]\n","print(processed[:30])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90EMOlDdlCm_","executionInfo":{"status":"ok","timestamp":1738294175989,"user_tz":-330,"elapsed":27,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"4abc5982-75a2-43ea-9a32-fb524a03dbcf"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"]}]},{"cell_type":"markdown","source":["### Creating Token IDs for splitted words(tokens)"],"metadata":{"id":"BSsQ3cx2uK10"}},{"cell_type":"code","source":["all_words = sorted(set(processed))\n","print(f\"Number of unique words: {len(all_words)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvDcjqUYoiYT","executionInfo":{"status":"ok","timestamp":1738294175989,"user_tz":-330,"elapsed":26,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"a81fb580-9094-4608-ddf4-a6135c0e15af"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique words: 1130\n"]}]},{"cell_type":"code","source":["vocab = {token:integer for integer, token in enumerate(all_words)}"],"metadata":{"id":"QdsUw9uCuUsD","executionInfo":{"status":"ok","timestamp":1738294175990,"user_tz":-330,"elapsed":25,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["for i, items in enumerate(vocab.items()):\n","  print(items)\n","  if i >= 50:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEDhqd1-u3Mi","executionInfo":{"status":"ok","timestamp":1738294175990,"user_tz":-330,"elapsed":25,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"1ce40065-aab0-419c-d6b2-016984d7d753"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["('!', 0)\n","('\"', 1)\n","(\"'\", 2)\n","('(', 3)\n","(')', 4)\n","(',', 5)\n","('--', 6)\n","('.', 7)\n","(':', 8)\n","(';', 9)\n","('?', 10)\n","('A', 11)\n","('Ah', 12)\n","('Among', 13)\n","('And', 14)\n","('Are', 15)\n","('Arrt', 16)\n","('As', 17)\n","('At', 18)\n","('Be', 19)\n","('Begin', 20)\n","('Burlington', 21)\n","('But', 22)\n","('By', 23)\n","('Carlo', 24)\n","('Chicago', 25)\n","('Claude', 26)\n","('Come', 27)\n","('Croft', 28)\n","('Destroyed', 29)\n","('Devonshire', 30)\n","('Don', 31)\n","('Dubarry', 32)\n","('Emperors', 33)\n","('Florence', 34)\n","('For', 35)\n","('Gallery', 36)\n","('Gideon', 37)\n","('Gisburn', 38)\n","('Gisburns', 39)\n","('Grafton', 40)\n","('Greek', 41)\n","('Grindle', 42)\n","('Grindles', 43)\n","('HAD', 44)\n","('Had', 45)\n","('Hang', 46)\n","('Has', 47)\n","('He', 48)\n","('Her', 49)\n","('Hermia', 50)\n"]}]},{"cell_type":"markdown","source":["Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n","    \n","Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n","\n","Step 3: Process input text into token IDs\n","\n","Step 4: Convert token IDs back into text\n","\n","Step 5: Replace spaces before the specified punctuation\n"],"metadata":{"id":"BjSnnbTA2NtS"}},{"cell_type":"code","source":["class SimpleTokenizerV1:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","    def encode(self, text):\n","        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","\n","        preprocessed = [\n","            item.strip() for item in preprocessed if item.strip()\n","        ]\n","        ids = [self.str_to_int[s] for s in preprocessed]\n","        return ids\n","\n","    def decode(self, ids):\n","        text = \" \".join([self.int_to_str[i] for i in ids])\n","        # Replace spaces before the specified punctuations\n","        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","        return text"],"metadata":{"id":"BAUmuCGMu4DS","executionInfo":{"status":"ok","timestamp":1738294175990,"user_tz":-330,"elapsed":24,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV1(vocab)\n","\n","text = \"\"\"\"It's the last he painted, you know,\"\n","           Mrs. Gisburn said with pardonable pride.\"\"\"\n","\n","ids = tokenizer.encode(text)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxOQuzh53kCD","executionInfo":{"status":"ok","timestamp":1738294175991,"user_tz":-330,"elapsed":24,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"4fa9809f-8c4b-48fd-bc20-a3bbc09e4c6e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rj2saF3M8Z1e","executionInfo":{"status":"ok","timestamp":1738294175991,"user_tz":-330,"elapsed":22,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"338b15be-2ac0-456d-a436-a17dc0675b8d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"]}]},{"cell_type":"code","source":["text = \"Hello, how you doing ?\"\n","tokenizer.encode(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"S9Xh5NjY9qSV","executionInfo":{"status":"error","timestamp":1738294175991,"user_tz":-330,"elapsed":20,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"80389add-2d43-4436-fb58-ffbf93feb5cb"},"execution_count":14,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'Hello'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-d8e682549191>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, how you doing ?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-ce18d2cbde5a>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-ce18d2cbde5a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Hello'"]}]},{"cell_type":"markdown","source":["We can modify the tokenizer to use an <|unk|> token if it\n","encounters a word that is not part of the vocabulary.\n","\n","Furthermore, we add a token between\n","unrelated texts.\n","\n","For example, when training GPT-like LLMs on multiple independent\n","documents or books, it is common to insert a token before each document or book that\n","follows a previous text source\n","\n","\n"],"metadata":{"id":"h7vN1r14G42y"}},{"cell_type":"markdown","source":["Let's now modify the vocabulary to include these two special tokens, <unk> and\n","<|endoftext|>, by adding these to the list of all unique words that we created in the\n","previous section:"],"metadata":{"id":"hrcICPCgG-m3"}},{"cell_type":"code","source":["all_tokens = sorted(set(processed))\n","all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])"],"metadata":{"id":"WbP3CXvc-n3C","executionInfo":{"status":"ok","timestamp":1738294213082,"user_tz":-330,"elapsed":478,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["vocab = {token:integer for integer, token in enumerate(all_tokens)}"],"metadata":{"id":"LxCCI-yYHkir","executionInfo":{"status":"ok","timestamp":1738294213446,"user_tz":-330,"elapsed":2,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["len(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSZQZO8RHlFY","executionInfo":{"status":"ok","timestamp":1738294214007,"user_tz":-330,"elapsed":5,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"149ef1c4-98d3-47e5-e874-093942f316c4"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1132"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["for i, j in enumerate(list(vocab.items())[-5:]):\n","  print(j)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWD1DdmyIrWy","executionInfo":{"status":"ok","timestamp":1738294214007,"user_tz":-330,"elapsed":4,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"3972a756-c20a-4d3e-8906-a967d350f7dc"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["('younger', 1127)\n","('your', 1128)\n","('yourself', 1129)\n","('<|endoftext|>', 1130)\n","('<|unk|>', 1131)\n"]}]},{"cell_type":"markdown","source":["### A simple text tokenizer that handles unknown words"],"metadata":{"id":"m1AHN2EoJ2Vy"}},{"cell_type":"markdown","source":["Step 1: Replace unknown words by <|unk|> tokens\n","    \n","Step 2: Replace spaces before the specified punctuations\n"],"metadata":{"id":"N0vCAXvwJ6hx"}},{"cell_type":"code","source":["class SimpleTokenizerV2:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","    preprocessed = [\n","        item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n","    ]\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[i] for i in ids])\n","    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"FJwAr5UcJBgU","executionInfo":{"status":"ok","timestamp":1738294214898,"user_tz":-330,"elapsed":3,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV2(vocab)\n","\n","text1 = \"Hello, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","\n","text = \" <|endoftext|> \".join((text1, text2))\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NjVUmPJLCTm","executionInfo":{"status":"ok","timestamp":1738294214898,"user_tz":-330,"elapsed":3,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"9602d78f-32a6-430d-fb6f-486f8cadce66"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"]}]},{"cell_type":"code","source":["tokenizer.encode(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uUc7wYERLQd7","executionInfo":{"status":"ok","timestamp":1738294215421,"user_tz":-330,"elapsed":6,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"bd90f22c-3e1d-4f71-a2c8-66fa9317e426"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["tokenizer.decode(tokenizer.encode(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"HWsoC4LTLbKM","executionInfo":{"status":"ok","timestamp":1738294216069,"user_tz":-330,"elapsed":5,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"dd6b8ee1-0123-402d-bd3e-70f3362762dc"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["[BOS] (beginning of sequence): This token marks the start of a text. It\n","signifies to the LLM where a piece of content begins.\n","\n","[EOS] (end of sequence): This token is positioned at the end of a text,\n","and is especially useful when concatenating multiple unrelated texts,\n","similar to <|endoftext|>. For instance, when combining two different\n","Wikipedia articles or books, the [EOS] token indicates where one article\n","ends and the next one begins.\n","\n","[PAD] (padding): When training LLMs with batch sizes larger than one,\n","the batch might contain texts of varying lengths. To ensure all texts have\n","the same length, the shorter texts are extended or \"padded\" using the\n","[PAD] token, up to the length of the longest text in the batch."],"metadata":{"id":"sC-J-GRxOuZQ"}},{"cell_type":"markdown","source":["###**Byte Pair encoding**"],"metadata":{"id":"ANKEMnZKfPzU"}},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"id":"G8ZgNJWCMhoB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738294220883,"user_tz":-330,"elapsed":3498,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"2c1dbb28-649a-47a5-ad99-aa56a54e438f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n","Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.8.0\n"]}]},{"cell_type":"code","source":["import tiktoken"],"metadata":{"id":"_-ieVANNfPRk","executionInfo":{"status":"ok","timestamp":1738294220883,"user_tz":-330,"elapsed":4,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["tokenizer = tiktoken.get_encoding(\"gpt2\")"],"metadata":{"id":"5LCAJGPcfUxx","executionInfo":{"status":"ok","timestamp":1738294222200,"user_tz":-330,"elapsed":1321,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n","     \"of someunknownPlace.\"\n",")\n","\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","print(integers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"daoVZkS-fZ29","executionInfo":{"status":"ok","timestamp":1738294222201,"user_tz":-330,"elapsed":3,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"a47ab3ef-4358-4d97-a5ce-fb28bb5f2126"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"]}]},{"cell_type":"code","source":["string = tokenizer.decode(integers)"],"metadata":{"id":"ePLSKVD-f9ub","executionInfo":{"status":"ok","timestamp":1738294654842,"user_tz":-330,"elapsed":569,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["print(string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GiCfNsjib3D0","executionInfo":{"status":"ok","timestamp":1738294661462,"user_tz":-330,"elapsed":362,"user":{"displayName":"hitesh ram","userId":"07694347647693359982"}},"outputId":"3ce0fa0a-dae1-4293-a0c4-7a1449b3e67e"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qIFKXdyzb4AF"},"execution_count":null,"outputs":[]}]}